{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "\n",
    "import base64\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pystac\n",
    "import requests\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import seaborn as sns\n",
    "import folium\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "from distutils.command import sdist\n",
    "from folium import plugins\n",
    "from matplotlib import pyplot as plt\n",
    "from pyproj.crs import CRS\n",
    "from pystac_client import Client\n",
    "from pprint import pprint\n",
    "from shapely.geometry import box, mapping, Point, Polygon\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Project specific packages\n",
    "from FH_Hydrosat import FH_StackedDataset\n",
    "from FH_Hydrosat import FH_Hydrosat\n",
    "from herbie import FastHerbie\n",
    "# import synoptic.services as ss\n",
    "# from synoptic.services import stations_timeseries\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK FUNCTIONS\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def stac_tile_search(collection, geom, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Log into STAC and search for a specified image collection.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    collection: list\n",
    "        List of tiles found in STAC.\n",
    "\n",
    "    geom: \n",
    "        Point location to search.\n",
    "\n",
    "    start_date, end_date: str\n",
    "        Dates to search between.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # number_dates = start_date - end_date\n",
    "\n",
    "    # if number_dates > 1000:\n",
    "    #    raise Exception(\"The nuber of dates requested in stac_tile_search() has exceeded 1000.\")\n",
    "\n",
    "    search = catalog.search(\n",
    "        collections=collection,\n",
    "        intersects=geom,\n",
    "        datetime=[start_date, end_date],\n",
    "        max_items=1000,\n",
    "    )\n",
    "\n",
    "    # items = list(search.items()) # for pystac-client >= 0.4.0\n",
    "    found_items = list(search.get_all_items())  # for pystac-client < 0.4.0\n",
    "\n",
    "    # Filter out only the newest version of MODIS.\n",
    "    if collection == 'prepped_inputs_mcd43a4':\n",
    "        version_str = '061'\n",
    "        new_list = [i for i in found_items if version_str in i.id]\n",
    "        found_items = new_list\n",
    "\n",
    "    found_items.reverse()  # make the results ascending in time\n",
    "\n",
    "    num_tiles = len(found_items)\n",
    "    print(\"Colllection: {}.  {} Images found.\".format(collection, num_tiles))\n",
    "\n",
    "    return (found_items, num_tiles)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def create_clip_polygon(geom, ds, buffer):\n",
    "    \"\"\"\n",
    "    Create a polygon that we will use for clipping the big dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    geom: dict\n",
    "        Dictionary of lat and lon of center point of AOI.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    poly: dataframe\n",
    "        Contains geometry of a square to be used for clipping.\n",
    "    \"\"\"\n",
    "    # Using the point coords defined earlier, create a df with the point geometry.\n",
    "    p_geom = Point(geom['coordinates'][0], geom['coordinates'][1])\n",
    "    point_df = gpd.GeoDataFrame(\n",
    "        {'geometry': [p_geom]}, crs=CRS.from_epsg(4326))\n",
    "\n",
    "    # Define a buffer size (for each side of the point.\n",
    "    # Reproject the point df and create the new polygon.\n",
    "    raster_crs = CRS.from_wkt(ds.spatial_ref.crs_wkt)\n",
    "    buffer_dist = buffer  # 1km in local UTM zone\n",
    "\n",
    "    # create a square buffer\n",
    "    poly_df = point_df.to_crs(raster_crs).buffer(buffer_dist, cap_style=3)\n",
    "\n",
    "    return (poly_df)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def create_aoi_image_stack(asset, items, num_tiles, geom, buffer):\n",
    "    '''\n",
    "    Gets images, stacks them and sorts them by date and clips them down to a smaller\n",
    "    AOI size.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    itmes: list \n",
    "        List of available images.\n",
    "    num_tiles: int\n",
    "        Number of tiles to download (days)\n",
    "    asset: str\n",
    "        Name of asset to get.\n",
    "    geom: x,y coords\n",
    "        Coordinates around which to build a polygon\n",
    "    buffer: int\n",
    "        Buffer around the x,y for creating the AOI rectangele.  In meters.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    aoi_stack_ds: dataset as FH_StackedDataset object.\n",
    "        Stack of images clipped to AOI.\n",
    "\n",
    "    '''\n",
    "    images = FH_Hydrosat(items[:num_tiles], asset=asset)\n",
    "\n",
    "    # Stacks all the files into a dataset and then return a FH_StackedDataset object.\n",
    "    stacked_images = images.stack()\n",
    "    # Sort the dataset by time.\n",
    "    ds = stacked_images.ds.sortby('time')\n",
    "\n",
    "    # Create polygon Area of Interest (AOI for which to sample.).\n",
    "    clip_poly_df = create_clip_polygon(geom, ds, buffer=buffer)\n",
    "    # Use AOI polygon to clip the dataset dwon to size and make it into a FH_StackedDataset object.\n",
    "    clipped = FH_StackedDataset(ds.rio.clip(clip_poly_df.geometry))\n",
    "    aoi_stack_ds = clipped.ds\n",
    "\n",
    "    return (aoi_stack_ds, clip_poly_df)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def extract_time_series(items, asset, bbox, tol, pad, band, var_name):\n",
    "    '''\n",
    "    Uses FH_Hydrosat class method point_time_series_from_items()\n",
    "    to extract only a time-series.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    items: list\n",
    "        Image items returned from STAC search.\n",
    "    bbox: \n",
    "        Bounding box of coordinates for seacrh site.\n",
    "    tol: int\n",
    "        A search parameter in meters for finding point data.\n",
    "    var_name: str\n",
    "        Dataframe column name for data extracted.\n",
    "    asset: str\n",
    "        Search parameter for type of asset to be searched.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    lst_df: dataframe\n",
    "        Dataframe containing date time series.\n",
    "    '''\n",
    "    # Sample the LST items.\n",
    "    lst_res = FH_Hydrosat(items, asset=asset)\n",
    "\n",
    "    # Set the point for time-series extraction.\n",
    "    point_wgs84 = Point(box(*bbox).centroid.x, box(*bbox).centroid.y)\n",
    "\n",
    "    # Extract time-series data using function.\n",
    "    # band needs to be an int because it comes in as a string.\n",
    "    band = int(band)\n",
    "    lst_k = lst_res.point_time_series_from_items(\n",
    "        point_wgs84, tol=tol, nproc=6, band=band)\n",
    "\n",
    "    # Create a datetime dataframe\n",
    "    lst_dt = lst_res.datetime\n",
    "    lst_df = pd.DataFrame({var_name: lst_k,\n",
    "                           'datetime': pd.to_datetime(lst_dt)}).sort_values(by='datetime')\n",
    "\n",
    "    # Get the date in the correct/consistent format.\n",
    "    lst_df['date'] = [t.to_pydatetime().strftime('%Y-%m-%d')\n",
    "                      for t in lst_df['datetime']]\n",
    "    lst_df['date'] = pd.to_datetime(lst_df['date'])\n",
    "    lst_df.drop(columns='datetime', inplace=True)\n",
    "    lst_df.set_index('date', drop=True, inplace=True)\n",
    "\n",
    "    return (lst_df)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def get_hrrr_point_data(lat, lon, start, days):\n",
    "    ''' \n",
    "    Fetch met data from the HRRR model system.\n",
    "    Herbie package needs to be installed:\n",
    "    https://herbie.readthedocs.io/en/stable/\n",
    "    ${HOME}/.config/herbie/config.toml\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    lat, lon: float\n",
    "        Coords from which to pull grid cell data. \n",
    "    start: str\n",
    "        This is the start date for which data will be retrieved.\n",
    "    days: int\n",
    "        Number of days of data to retrieve.\n",
    "\n",
    "    Return:\n",
    "    -------\n",
    "    ds_point: xarray dataset\n",
    "        Met data for specified point.\n",
    "    '''\n",
    "\n",
    "    # Create a range of dates\n",
    "    DATES = pd.date_range(\n",
    "        start=start,\n",
    "        periods=days,\n",
    "        freq=\"1D\",\n",
    "    )\n",
    "\n",
    "    # Define forecast lead time (or analysis).\n",
    "    fxx = range(0, 1)\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # For long time-series analyses\n",
    "    # -----------------------------------------------\n",
    "    FH = FastHerbie(DATES, model=\"hrrr\", fxx=fxx)\n",
    "    FH.download(\"TMP:2 m\")\n",
    "    ds = FH.xarray(\"TMP:2 m\", remove_grib=False)\n",
    "    # -----------------------------------------------\n",
    "    # For real-time fetching of HRRR data.\n",
    "    # -----------------------------------------------\n",
    "    # Make FastHerbie Object.\n",
    "    # FH = FastHerbie(DATES, model=\"hrrr\", fxx=fxx)\n",
    "    # Read a subset of the data with xarray.\n",
    "    # ds = FH.xarray(\"TMP:2 m\", remove_grib=False)\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    # Get data values nearest single point\n",
    "    ds_point = ds.herbie.nearest_points(points=(lon, lat))\n",
    "\n",
    "    return (ds_point)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def read_ameriflux(data_path):\n",
    "\n",
    "    print('Reading file {}'.format(data_path))\n",
    "\n",
    "    df = pd.read_csv(data_path, header=0, na_values=[-9999.000000])\n",
    "\n",
    "    # Save value column names\n",
    "    value_cols = df.columns[2:]\n",
    "\n",
    "    # Convert timestamp objects\n",
    "    df['start'] = df['TIMESTAMP_START'].apply(\n",
    "        lambda x: datetime.strptime(str(x), \"%Y%m%d%H%M.0\")\n",
    "    )\n",
    "    df['end'] = df['TIMESTAMP_END'].apply(\n",
    "        lambda x: datetime.strptime(str(x), \"%Y%m%d%H%M.0\")\n",
    "    )\n",
    "\n",
    "    # Convert obs to UTC time.txt.\n",
    "    # # UTC_OFFSET is a global var.\n",
    "    df['start'] = df['start'] + timedelta(hours=UTC_OFFSET)\n",
    "    df['end'] = df['end'] + timedelta(hours=UTC_OFFSET)\n",
    "    df['start'] = df['start'].dt.tz_localize('UTC')\n",
    "    df['end'] = df['end'].dt.tz_localize('UTC')\n",
    "\n",
    "    # Drop NA\n",
    "    df = df.dropna(subset=value_cols, how='all')\n",
    "\n",
    "    df = df.set_index('start')\n",
    "    col_order = (['end', 'TIMESTAMP_START', 'TIMESTAMP_END']\n",
    "                 + value_cols.to_list())\n",
    "    df = df[col_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def get_ameriflux_met_dates(start_dt, end_dt, image_dates):\n",
    "\n",
    "    df3 = pd.DataFrame()\n",
    "\n",
    "    met_path = os.path.join(\n",
    "        data_path, 'US-Bi2_HH_201704270000_202301010000.csv')\n",
    "    \n",
    "    # Read file.\n",
    "    met_df = read_ameriflux(met_path)\n",
    "\n",
    "    # Extract variables into dataframes.\n",
    "    ta_df = met_df['TA'] + 273\n",
    "    ta_df = ta_df.to_frame()\n",
    "    vpd_df = met_df['VPD'].to_frame()\n",
    "    swc16_df = met_df['Soil water content of sensor 1 at 16cm'].to_frame()\n",
    "    swc26_df = met_df['Soil water content of sensor 2 at 26cm'].to_frame()\n",
    "    \n",
    "    ta_df.columns = ['TA']\n",
    "    vpd_df.columns = ['VPD']\n",
    "    swc26_df.columns = ['SWC26']\n",
    "\n",
    "    df1 = ta_df.merge(vpd_df, left_index=True, right_index=True, how='outer').merge(swc26_df, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    df1 = df1.loc[start_dt:end_dt]\n",
    "    df1.index = pd.to_datetime(df1.index)\n",
    "\n",
    "    match_timestamp = \"18:30:00\"\n",
    "    df2 = df1.loc[df1.index.strftime(\"%H:%M:%S\") == match_timestamp]\n",
    "    df3 = df2  #.to_frame()\n",
    "\n",
    "    df3['time'] = df3.index.strftime('%Y-%m-%d')\n",
    "    df3['time'] = pd.to_datetime(df3['time'])\n",
    "    df3.set_index('time', inplace=True, append=True)\n",
    "    \n",
    "    ### temp_df = df3[df3.index.get_level_values('date').isin(image_dates.index)]\n",
    "    \n",
    "    ### print (df3.index.get_level_values('date').isin(image_dates.index))\n",
    "    ### temp_df.reset_index(level=[0], inplace=True)\n",
    "    # return (temp_df)\n",
    "    df3.reset_index(level=[0], inplace=True)\n",
    "\n",
    "    return (df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAC LOGIN\n",
    "\n",
    "# Open credentials file.\n",
    "with open('../secrets/creds.json') as f:\n",
    "    creds = json.loads(f.read())\n",
    "\n",
    "# Endecode the `username:password` combination\n",
    "# and use it to authorize access to the STAC API given by the `cat_url`\n",
    "# endpoint.userpass = f\"{creds['username']}:{creds['password']}\"\n",
    "userpass = f\"{creds['username']}:{creds['password']}\"\n",
    "b64 = base64.b64encode(userpass.encode()).decode()\n",
    "headers = {'Authorization': 'Basic ' + b64}\n",
    "\n",
    "cat_url = 'https://fusion-stac.hydrosat.com'\n",
    "catalog = Client.open(cat_url, headers)\n",
    "\n",
    "if not catalog.id == 'stac-server':\n",
    "    print(\"You have failed to log into the STAC server\\n\")\n",
    "else:\n",
    "    print(\"You are succesfully logged in to the STAC server and you can now begin STAC queries.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS SETUP\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# USER EDITS REQUIRED -------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "analysis = 'Corn'\n",
    "\n",
    "# Want met obs in UTC time to match satellites.\n",
    "UTC_OFFSET = 7\n",
    "\n",
    "# Insitu point locations\n",
    "insitu_met_points = {\n",
    "    'Corn': [38.1091, -121.5351],\n",
    "    'Alfalfa': [38.0992, -121.4993]\n",
    "}\n",
    "\n",
    "# Center point of crop fields.\n",
    "crop_center_points = {\n",
    "    'Corn': [-121.5360, 38.1095],\n",
    "    'Alfalfa': [-121.5015, 38.0994],\n",
    "    'Ames': [-93.701, 42.000],\n",
    "}\n",
    "\n",
    "# Set dates for corn where ndvi >= 0.5.\n",
    "# Dates determined in a diff notebook called\n",
    "# lst-point-area-comparison-tcruicks-ipynb.\n",
    "start1 = '2021-05-01T00:00:00Z'\n",
    "end1 = '2021-10-31T00:00:00Z'\n",
    "\n",
    "start2 = '2022-05-01T00:00:00Z'\n",
    "end2 = '2022-10-31T00:00:00Z'\n",
    "\n",
    "# Create a DataFrame with all dates between specified start<-->end using pd.date_range()\n",
    "# We'll use this later on to create full date dataframes with data.\n",
    "all_dates = pd.DataFrame(pd.date_range(\n",
    "    '2021-01-01', '2022-12-31', name='date'))\n",
    "\n",
    "# Set the index and drop the duplicate date column.\n",
    "all_dates.set_index(pd.to_datetime(all_dates.date), inplace=True)\n",
    "all_dates.drop(columns='date', inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Define paths to met data\n",
    "data_path = os.path.join('../data', 'Ameriflux')\n",
    "\n",
    "if analysis == 'Alfalfa':\n",
    "    bbox = [-121.5027, 38.0986, -121.5003, 38.1000]\n",
    "    met_path = os.path.join(data_path,\n",
    "                            'US-Bi1_HH_201608130000_202301010000.csv')\n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"You do not have the required Ameriflux data file for a {} analysis\".format(\n",
    "            analysis))\n",
    "\n",
    "elif analysis == 'Corn':\n",
    "    bbox = [-121.5365, 38.1098, -121.5355, 38.1091]\n",
    "    met_path = os.path.join(data_path,\n",
    "                            'US-Bi2_HH_201704270000_202301010000.csv')\n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"You do not have the required Ameriflux data file for a {} analysis\".format(\n",
    "            analysis))\n",
    "\n",
    "# Create dict of coords.  Will be used to create a polygon for our AOI.\n",
    "geom = {'type': 'Point', 'coordinates': crop_center_points[analysis]}\n",
    "\n",
    "print(\"Analysis setup comlete.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTINEL IMAGE SEARCH\n",
    "collections = [\"prepped_inputs_s2\"]\n",
    "\n",
    "# Search STAC for available images.\n",
    "(s2_items1, num_tiles1) = stac_tile_search(collections, geom, start1, end1)\n",
    "(s2_items2, num_tiles2) = stac_tile_search(collections, geom, start2, end2)\n",
    "s2_items = s2_items1 + s2_items2\n",
    "num_s2_tiles = num_tiles1 + num_tiles2\n",
    "\n",
    "print(f\"First date: {s2_items[0]}\")\n",
    "print(f\"Last date: {s2_items[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT SURFACE REFLECTANCE & COMPUTE NDVI\n",
    "\n",
    "try:\n",
    "    (aoi_stack_ds, clip_poly_df) = create_aoi_image_stack(\n",
    "        'surface_reflectance', s2_items, num_s2_tiles, geom, 65)\n",
    "except:\n",
    "    print(\"Your STAC search turned up {} images.\".format(num_s2_tiles))\n",
    "    print(\"You'll need to modify your STACsearch parameters.\\n\")\n",
    "\n",
    "# Pull out NIR and Red bands.\n",
    "red_aoi_ds = aoi_stack_ds.isel(band=2)\n",
    "nir_aoi_ds = aoi_stack_ds.isel(band=6)\n",
    "\n",
    "# Compute NDVI.\n",
    "ndvi_aoi_ds = (nir_aoi_ds - red_aoi_ds) / (nir_aoi_ds + red_aoi_ds)\n",
    "\n",
    "# Create a NDVI AOI mean dataset.\n",
    "mean_ndvi_aoi_ds = ndvi_aoi_ds.mean(dim=[\"x\", \"y\"])\n",
    "\n",
    "# Create a NDVI AOI variance dataset.\n",
    "var_ndvi_aoi_ds = ndvi_aoi_ds.var(dim=[\"x\", \"y\"])\n",
    "\n",
    "# Convert dataasets to dataframes.\n",
    "df = ndvi_aoi_ds.to_dataframe(name='s2_ndvi')\n",
    "mean_df = mean_ndvi_aoi_ds.to_dataframe(name='s2_mean_ndvi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2021-05-02 00:00:00+00:00', '2021-05-03 00:00:00+00:00',\n",
       "               '2021-05-04 00:00:00+00:00', '2021-05-05 00:00:00+00:00',\n",
       "               '2021-05-06 00:00:00+00:00', '2021-05-07 00:00:00+00:00',\n",
       "               '2021-05-08 00:00:00+00:00', '2021-05-09 00:00:00+00:00',\n",
       "               '2021-05-10 00:00:00+00:00', '2021-05-11 00:00:00+00:00',\n",
       "               ...\n",
       "               '2022-10-20 00:00:00+00:00', '2022-10-21 00:00:00+00:00',\n",
       "               '2022-10-22 00:00:00+00:00', '2022-10-23 00:00:00+00:00',\n",
       "               '2022-10-24 00:00:00+00:00', '2022-10-25 00:00:00+00:00',\n",
       "               '2022-10-26 00:00:00+00:00', '2022-10-27 00:00:00+00:00',\n",
       "               '2022-10-28 00:00:00+00:00', '2022-10-29 00:00:00+00:00'],\n",
       "              dtype='datetime64[ns, UTC]', name='time', length=22932, freq=None)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RESAMPLE NDVI DATAFRAME TO Y-M-D\n",
    "\n",
    "# Drop uneeded columns.\n",
    "# df.drop(columns=['spatial_ref'], inplace=True)\n",
    "\n",
    "# Convert multilevel index to single level.\n",
    "# df = df.reset_index(level=[1, 2])\n",
    "\n",
    "# Put index into dt64 format.\n",
    "# df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Resample to just the y-m-d and take the mean() which in this case\n",
    "# will be the mean ndvi for the AOI.\n",
    "resampled_df = df.groupby(['x','y']).resample('D', origin=pd.to_datetime('2021-05-01', utc=df.index.tz)).mean() # * This is great but then we loose all the individual pixel records.\n",
    "resampled_df.loc[:,:,'2021-05-02 00:00:00+00:00']\n",
    "\n",
    "resampled_df.drop(columns=['y','x'], inplace=True)\n",
    "resampled_df = resampled_df.reset_index(level=[0, 1])\n",
    "resampled_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOXPLOT NDVI AOI PIXELS\n",
    "\n",
    "from numpy import dtype\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "# Make box plots.\n",
    "box = (\n",
    "    sns.boxplot(\n",
    "    # x=s2_ndvi_df.index.to_series().apply(lambda x: x.strftime('%Y-%m-%d')),\n",
    "    x=resampled_df['2021-05-01':'2021-10-31'].index, #.strftime('%Y-%m-%d'),\n",
    "    y=resampled_df['2021-05-01':'2021-10-31']['s2_ndvi'],\n",
    "    ax=ax1)\n",
    "    .set(title=\"Sentinel 2 NDVI : AOI: 130 m2\"),\n",
    "    )\n",
    "\n",
    "#ax1.plot(resampled_df['2021-05-01':'2021-10-31'].index.strftime('%Y-%m-%d'), resampled_df['2021-05-01':'2021-10-31']['s2_ndvi'])\n",
    "\n",
    "ax1.set_ylabel('NDVI')\n",
    "#ax1.xaxis.set_major_locator(mdates.MonthLocator(bymonthday=1))\n",
    "ax1.tick_params(labelrotation=25)\n",
    "\n",
    "#box.set(xlim=(pd.to_datetime('2021-05-01'), pd.to_datetime('2021-10-01')))\n",
    "\n",
    "ax1.set_xlim(datetime(2021,5,1,0,0,0), datetime(2021,10,1,0,0,0))\n",
    "\n",
    "ax1.grid()\n",
    "\n",
    "# ***** How do I make the xaxis go from 2021-05-01 to 2021-10-31?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET METEOROLOGY AMERIFLUX\n",
    "# Will use met_df in next code cell.\n",
    "met_df = read_ameriflux(met_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT LST FROM EACH COLLECTION\n",
    "\n",
    "# Dictionary keys are: var_name, tolerance, resolution, plot color.\n",
    "#     var_name: used as column name in dataframe.\n",
    "#     tolerance (m): search parameter in meters for extracting point data.\n",
    "#     resolution (m): this is the legend when plotting\n",
    "#     plot color: for plotting\n",
    "#     buffer (m): for clipping.\n",
    "\n",
    "# See https://hydrosat.github.io/fusion-hub-docs/3-FH-API-Spec.html\n",
    "# for asset specifications.\n",
    "\n",
    "# prepped_inputs_landsat = resampled to 30 m\n",
    "# prepped_inputs_mod21a1d = 1000 m\n",
    "# pydms_sharpened_modis = 500 m\n",
    "# pydms_sharpened_landsat = downscaled to 20 m\n",
    "# starfm_predictions_modis_landsat = 20 m\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# USER EDITS POSSIBLE -------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "asset_dict = {\n",
    "    #\"prepped_inputs_mod21a1d\": ['mod21a1d_lst', 2000, 1000, 'brown', 500],\n",
    "    #\"pydms_sharpened_modis\": ['pydms_modis_lst', 1000, 500, 'green', 250],\n",
    "    \"prepped_inputs_landsat\": ['lsat_lst', 60, 30, 'black', 65],\n",
    "    #\"pydms_sharpened_landsat\": ['pydms_lsat_lst', 40, 20, 'blue', 65],\n",
    "    #\"starfm_predictions_modis_landsat\": ['starfm_lst', 40, 20, 'red', 65],\n",
    "}\n",
    "# ---------------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "df_list = []  # A list that will hold each collections catd dataset.\n",
    "catd_list = [] # A list that will hold single timestep da's of catd.\n",
    "\n",
    "# Loop through the dictionary.\n",
    "for i, asset in enumerate(asset_dict):\n",
    "\n",
    "    # Just to make sure these lists get reset after each loop iteration.\n",
    "    found_items1 = []\n",
    "    found_items2 = []\n",
    "    found_items = []\n",
    "\n",
    "    # Search and retrieve images from the two seasons.\n",
    "    (found_items1, num_tiles1) = stac_tile_search(asset, geom, start2, end2)\n",
    "    #(found_items2, num_tiles2) = stac_tile_search(asset, geom, start2, end2)\n",
    "    found_items = found_items1 # + found_items2\n",
    "    num_tiles = num_tiles1 # + num_tiles2\n",
    "\n",
    "    # Extract LST data from AOI. ----------------\n",
    "    (aoi_lst_da, aoi_df) = create_aoi_image_stack(\n",
    "            'lst', found_items, num_tiles, geom, asset_dict[asset][4])\n",
    "    \n",
    "    # Want to process each timesteps CATD so we iterate through\n",
    "    # time and work with a single dimension at at time.\n",
    "    for date, aoi in aoi_lst_da.groupby('time'):\n",
    "\n",
    "        # Get TA from dataframe using each time step (date).\n",
    "        ta = met_df.TA.iloc[met_df.index.get_indexer([date], method='nearest')]\n",
    "        # COmpute CATD.  Subtract the TA value from each AOI pixel.\n",
    "        catd = aoi - ta.values\n",
    "        # Append new CATD da to a list.\n",
    "        catd_list.append(catd)\n",
    "\n",
    "    # After each date has been computed.  Concatenate them all together.\n",
    "    catd_ds = xr.concat(catd_list, coords='minimal', dim='time', compat='override').squeeze()\n",
    "\n",
    "    df_list.append(catd_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW METHOD FOR COMPUTING CATD\n",
    "\n",
    "catd_list=[]\n",
    "\n",
    "for date, aoi in aoi_lst_da.groupby('time'):\n",
    "\n",
    "    # get ta from dataframe\n",
    "    #can I do this with a single method?\n",
    "    ta = met_df.TA.iloc[met_df.index.get_indexer([date], method='nearest')]\n",
    " \n",
    "    catd = aoi - ta.values\n",
    "\n",
    "    catd_list.append(catd)\n",
    "    break\n",
    "\n",
    "catd_ds = xr.concat(catd_list, coords='minimal', dim='time', compat='override').squeeze()\n",
    "catd_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 6))\n",
    "\n",
    "try_da.plot(ax=ax1)\n",
    "org.plot(ax=ax2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ea-lst-alpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f8cb41e19b5dcf5f26e7d3640eb2927eaa0095f403f6e610552a8f30a9f6c69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
