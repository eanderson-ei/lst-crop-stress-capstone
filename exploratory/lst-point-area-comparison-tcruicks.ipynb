{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Detection of Crop Stress From Thermal Infrared Imagery\n",
    "![Hydrosat](https://uploads-ssl.webflow.com/61e4aee27ac4a95d23ab9609/61e9d6f5d6578e8c7c0cca8f_solutions-thermal-min.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pystac\n",
    "import requests\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from distutils.command import sdist\n",
    "from matplotlib import pyplot as plt\n",
    "from pyproj.crs import CRS\n",
    "from pystac_client import Client\n",
    "from pprint import pprint\n",
    "from shapely.geometry import box, mapping, Point, Polygon\n",
    "\n",
    "# Project specific packages\n",
    "from FH_Hydrosat import FH_StackedDataset\n",
    "from FH_Hydrosat import FH_Hydrosat\n",
    "from herbie import FastHerbie\n",
    "import synoptic.services as ss\n",
    "from synoptic.services import stations_timeseries\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS.  No user editing is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def create_clip_polygon(geom, ds, buffer):\n",
    "    \"\"\"\n",
    "    Create a polygon that we will use for clipping the big dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    geom: dict\n",
    "        Dictionary of lat and lon of center point of AOI.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    poly: dataframe\n",
    "        Contains geometry of a square to be used for clipping.\n",
    "    \"\"\"\n",
    "    # Using the point coords defined earlier, create a df with the point geometry.\n",
    "    p_geom = Point(geom['coordinates'][0], geom['coordinates'][1])\n",
    "    point_df = gpd.GeoDataFrame({'geometry':[p_geom]}, crs=CRS.from_epsg(4326))\n",
    "\n",
    "    # Define a buffer size (for each side of the point.\n",
    "    # Reproject the point df and create the new polygon.\n",
    "    raster_crs = CRS.from_wkt(ds.spatial_ref.crs_wkt)\n",
    "    buffer_dist = buffer # 1km in local UTM zone\n",
    "\n",
    "    # create a square buffer\n",
    "    poly_df = point_df.to_crs(raster_crs).buffer(buffer_dist, cap_style = 3) \n",
    "\n",
    "    return(poly_df)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def stac_tile_search(collection, geom, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Log into STAC and search for a specified image collection.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    collection: list\n",
    "        List of tiles found in STAC.\n",
    "\n",
    "    geom: \n",
    "        Point location to search.\n",
    "\n",
    "    start_date, end_date: str\n",
    "        Dates to search between.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    \"\"\"\n",
    "    search = catalog.search(\n",
    "        collections = collection,\n",
    "        intersects = geom,\n",
    "        datetime = [start_date, end_date],\n",
    "        max_items = 500\n",
    "    )\n",
    "\n",
    "    #items = list(search.items()) # for pystac-client >= 0.4.0\n",
    "    found_items = list(search.get_all_items()) # for pystac-client < 0.4.0\n",
    "\n",
    "    # Filter out only the newest version of MODIS.\n",
    "    if collection == 'prepped_inputs_mcd43a4':\n",
    "        version_str = '061'\n",
    "        new_list = [i for i in found_items if version_str in i.id]\n",
    "        found_items = new_list\n",
    "\n",
    "    found_items.reverse() # make the results ascending in time\n",
    "\n",
    "    num_tiles = len(found_items)\n",
    "    print (\"Searching {} colllection .... images available: {}\\n\".format(collection, num_tiles))\n",
    "\n",
    "    return (found_items, num_tiles)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def create_aoi_image_stack(items, num_tiles, geom, asset, buffer):\n",
    "    '''\n",
    "    Gets images, stacks them and sorts them by date and clips them down to a smaller\n",
    "    AOI size.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    itmes: list \n",
    "        List of available images.\n",
    "    num_tiles: int\n",
    "        Number of tiles to download (days)\n",
    "    asset: str\n",
    "        Name of asset to get.\n",
    "    geom: x,y coords\n",
    "        Coordinates around which to build a polygon\n",
    "    buffer: int\n",
    "        Buffer around the x,y for creating the AOI rectangele.  In meters.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    aoi_stack_ds: dataset as FH_StackedDataset object.\n",
    "        Stack of images clipped to AOI.\n",
    "\n",
    "    '''\n",
    "    images = FH_Hydrosat(items[:num_tiles], asset=asset)\n",
    "\n",
    "    # Stacks all the files into a dataset and then return a FH_StackedDataset object.\n",
    "    stacked_images = images.stack()\n",
    "    # Sort the dataset by time.\n",
    "    ds = stacked_images.ds.sortby('time')\n",
    "\n",
    "    # Create polygon Area of Interest (AOI for which to sample.).\n",
    "    clip_poly_df = create_clip_polygon(geom, ds, buffer=buffer)\n",
    "    # Use AOI polygon to clip the dataset dwon to size and make it into a FH_StackedDataset object.\n",
    "    clipped = FH_StackedDataset(ds.rio.clip(clip_poly_df.geometry))\n",
    "    aoi_stack_ds = clipped.ds\n",
    "\n",
    "    return (aoi_stack_ds)\n",
    "\n",
    " # ---------------------------------------------------------------------------------   \n",
    "def extract_time_series(items, bbox, tol, var_name, asset, band):\n",
    "    '''\n",
    "    Uses FH_Hydrosat class method point_time_series_from_items()\n",
    "    to extract only a time-series.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    items: list\n",
    "        Image items returned from STAC search.\n",
    "    bbox: \n",
    "        Bounding box of coordinates for seacrh site.\n",
    "    tol: int\n",
    "        A search parameter in meters for finding point data.\n",
    "    var_name: str\n",
    "        Dataframe column name for data extracted.\n",
    "    asset: str\n",
    "        Search parameter for type of asset to be searched.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    lst_df: dataframe\n",
    "        Dataframe containing date time series.\n",
    "    '''\n",
    "    # Sample the LST items.\n",
    "    lst_res = FH_Hydrosat(items, asset=asset)\n",
    "\n",
    "    # Set the point for time-series extraction.\n",
    "    point_wgs84 = Point(box(*bbox).centroid.x, box(*bbox).centroid.y)\n",
    "    print (\"----\", point_wgs84)\n",
    "    \n",
    "    # Extract time-series data using function.\n",
    "    band = int(band) # band needs to be an int because it comes in as a string.\n",
    "    lst_k  = lst_res.point_time_series_from_items(point_wgs84, tol=tol, nproc=6, pad=0, band=band) \n",
    "\n",
    "    # Create a datetime dataframe\n",
    "    lst_dt = lst_res.datetime\n",
    "    lst_df = pd.DataFrame({var_name: lst_k,\n",
    "                       'datetime': pd.to_datetime(lst_dt)}).sort_values(by='datetime')\n",
    "    \n",
    "    # Get the date in the correct/consistent format.\n",
    "    lst_df['date'] = [t.to_pydatetime().strftime('%Y-%m-%d') for t in lst_df['datetime']]\n",
    "    lst_df['date'] = pd.to_datetime(lst_df['date'])\n",
    "    lst_df.drop(columns='datetime', inplace=True)\n",
    "    lst_df.set_index('date', drop=True, inplace=True)\n",
    "    \n",
    "    return (lst_df)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def get_hrrr_point_data(lat, lon, start, days):\n",
    "    ''' \n",
    "    Fetch met data from the HRRR model system.\n",
    "    Herbie package needs to be installed:\n",
    "    https://herbie.readthedocs.io/en/stable/\n",
    "    ${HOME}/.config/herbie/config.toml\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    lat, lon: float\n",
    "        Coords from which to pull grid cell data. \n",
    "    start: str\n",
    "        This is the start date for which data will be retrieved.\n",
    "    days: int\n",
    "        Number of days of data to retrieve.\n",
    "\n",
    "    Return:\n",
    "    -------\n",
    "    ds_point: xarray dataset\n",
    "        Met data for specified point.\n",
    "    '''\n",
    "    start_hr_time = \"{} 17:30\".format(start)\n",
    "    # Create a range of dates\n",
    "    DATES = pd.date_range(\n",
    "        start=start_hr_time,\n",
    "        periods=days,\n",
    "        freq=\"1D\",\n",
    "    )\n",
    "\n",
    "    # Define forecast lead time (or analysis).\n",
    "    fxx = range(0, 1)\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # For long time-series analyses\n",
    "    # -----------------------------------------------\n",
    "    FH = FastHerbie(DATES, model=\"hrrr\", fxx=fxx)\n",
    "\n",
    "    FH.download(\"TMP:2 m\")\n",
    "    t_ds = FH.xarray(\"TMP:2 m\", remove_grib=False)\n",
    "\n",
    "    FH.download(\"DPT:2 m\")\n",
    "    td_ds = FH.xarray(\"DPT:2 m\", remove_grib=False)\n",
    "    # -----------------------------------------------\n",
    "    # For real-time fetching of HRRR data.\n",
    "    # -----------------------------------------------\n",
    "    # Make FastHerbie Object.\n",
    "    # FH = FastHerbie(DATES, model=\"hrrr\", fxx=fxx)\n",
    "    # Read a subset of the data with xarray.\n",
    "    # ds = FH.xarray(\"TMP:2 m\", remove_grib=False)\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    # Get data values nearest single point\n",
    "    t_ds_point = t_ds.herbie.nearest_points(points=(lon, lat))\n",
    "    td_ds_point = td_ds.herbie.nearest_points(points=(lon, lat))\n",
    "   \n",
    "    return (t_ds_point, td_ds_point)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "def get_synoptic_obs(station):\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Request a single station by listing the station id.  \n",
    "    # ---------------------------------------------------------------------\n",
    "    params = dict(\n",
    "        stid=station,\n",
    "        vars=['air_temp', 'dew_point_temperature'],\n",
    "        start=datetime(2020, 4, 1),\n",
    "        end=datetime(2020, 10, 30)\n",
    "    )\n",
    "    data = stations_timeseries(verbose='HIDE', **params)\n",
    "\n",
    "    # Filter data to get only 00z obs.\n",
    "    sfc_obs = data.loc[(data.index.hour == 17) & (data.index.minute == 30)]\n",
    "    sfc_obs.drop(columns='dew_point_temperature_set_1', inplace=True)\n",
    "    # convert datetime column to just date\n",
    "    sfc_obs['time'] = sfc_obs.index.date\n",
    "    sfc_obs.reset_index(inplace=True)\n",
    "    sfc_obs.set_index('time', inplace=True)\n",
    "\n",
    "    # Change date format to be consistent.\n",
    "    sfc_obs.index = pd.to_datetime(sfc_obs.index)\n",
    "    sfc_obs.index = sfc_obs.index.strftime('%Y-%m-%d')\n",
    "\n",
    "    return (sfc_obs)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def read_ameriflux(data_path):\n",
    "    df = pd.read_csv(data_path, header=2, na_values=[-9999])\n",
    "\n",
    "    # Save value column names\n",
    "    value_cols = df.columns[2:]\n",
    "\n",
    "    # Convert timestamp objects\n",
    "    df['start'] = df['TIMESTAMP_START'].apply(\n",
    "        lambda x: datetime.strptime(str(x), \"%Y%m%d%H%M\")\n",
    "        )\n",
    "    df['end'] = df['TIMESTAMP_END'].apply(\n",
    "        lambda x: datetime.strptime(str(x), \"%Y%m%d%H%M\")\n",
    "        )\n",
    "\n",
    "    # Drop NA\n",
    "    df = df.dropna(subset=value_cols, how='all')\n",
    "\n",
    "    df = df.set_index('start')\n",
    "    col_order = (['end', 'TIMESTAMP_START', 'TIMESTAMP_END'] \n",
    "                 + value_cols.to_list())\n",
    "    df = df[col_order]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to STAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open credentials file.\n",
    "with open('../secrets/creds.json') as f:\n",
    "    creds = json.loads(f.read())\n",
    "\n",
    "# Endecode the `username:password` combination \n",
    "# and use it to authorize access to the STAC API given by the `cat_url` \n",
    "# endpoint.userpass = f\"{creds['username']}:{creds['password']}\"\n",
    "userpass = f\"{creds['username']}:{creds['password']}\"\n",
    "b64 = base64.b64encode(userpass.encode()).decode()\n",
    "headers = {'Authorization':'Basic ' + b64}\n",
    "\n",
    "cat_url = 'https://fusion-stac.hydrosat.com'\n",
    "catalog = Client.open(cat_url, headers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up analysis.  User edits required in cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# USER EDITS REQUIRED -------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "analysis = 'Ames'\n",
    "\n",
    "# Insitu point locations\n",
    "insitu_met_points = {\n",
    "    'Rice': [38.1235, -121.5490],\n",
    "    'Corn': [38.1091, -121.5351],\n",
    "    'Alfalfa': [38.0992, -121.4993]\n",
    "}\n",
    "\n",
    "# Center point of crop fields.\n",
    "crop_center_points = {\n",
    "    'Rice': [38.123, -121.550],\n",
    "    'Corn': [38.109, -121.536],\n",
    "    'Alfalfa': [38.099, -121.501]\n",
    "}\n",
    "\n",
    "bbox = [-121.52, 38.11, -121.54, 38.09]\n",
    "\n",
    "# Define size of AOI.\n",
    "#    Length    x    Width \n",
    "# (buffer * 2) x (buffer * 2)\n",
    "buffer = 40\n",
    "\n",
    "# Specify dates & hour for LST analysis.\n",
    "# start_date = \"2016-08-13T00:00:00Z\"\n",
    "# end_date = \"2022-05-26T00:00:00Z\"\n",
    "start = \"2016-05-01\"  \n",
    "end = \"2016-8-30\"  \n",
    "hr_s = \"00:00:00\"  \n",
    "hr_e = '23:59:59'\n",
    "# ---------------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Create dict of coords.  Will be used to create a polygon for our AOI.\n",
    "geom = {'type': 'Point', 'coordinates': locations[analysis]} \n",
    "\n",
    "# We need the total num of days for the analysis to get hrrr data.\n",
    "f = start.split(\"-\")\n",
    "l = end.split(\"-\")\n",
    "\n",
    "f_dt = date(int(f[0]), int(f[1]), int(f[2]))  \n",
    "l_dt = date(int(l[0]), int(l[1]), int(l[2]))  \n",
    "num_days = (l_dt - f_dt)\n",
    "num_days = num_days.days\n",
    "\n",
    "# Need to do some formating for various requirements.\n",
    "hr_s_form = \"T{}Z\".format(hr_s)\n",
    "hr_e_form = \"T{}Z\".format(hr_e)\n",
    "start_date = start + hr_s_form\n",
    "end_date = end + hr_e_form\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get time-series of Fused LST data from an x,y point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching ['starfm_predictions_modis_landsat', 'pydms_sharpened_landsat'] colllection .... images available: 0\n",
      "\n",
      "---- <class 'list'>\n",
      "---- POINT (-121.52999999999999 38.10000000000001)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m----\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtype\u001b[39m(bbox))\n\u001b[1;32m     11\u001b[0m \u001b[39m# Extract data from point.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m (lst_time_series_df) \u001b[39m=\u001b[39m (extract_time_series(found_items, bbox, \u001b[39mint\u001b[39;49m(\u001b[39m40\u001b[39;49m), \u001b[39m'\u001b[39;49m\u001b[39mfused_lst\u001b[39;49m\u001b[39m'\u001b[39;49m, asset, \u001b[39mint\u001b[39;49m(\u001b[39m0\u001b[39;49m)))\n",
      "Cell \u001b[0;32mIn[178], line 148\u001b[0m, in \u001b[0;36mextract_time_series\u001b[0;34m(items, bbox, tol, var_name, asset, band)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m# Extract time-series data using function.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m band \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(band) \u001b[39m# band needs to be an int because it comes in as a string.\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m lst_k  \u001b[39m=\u001b[39m lst_res\u001b[39m.\u001b[39;49mpoint_time_series_from_items(point_wgs84, tol\u001b[39m=\u001b[39;49mtol, nproc\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m, pad\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, band\u001b[39m=\u001b[39;49mband) \n\u001b[1;32m    150\u001b[0m \u001b[39m# Create a datetime dataframe\u001b[39;00m\n\u001b[1;32m    151\u001b[0m lst_dt \u001b[39m=\u001b[39m lst_res\u001b[39m.\u001b[39mdatetime\n",
      "File \u001b[0;32m~/earth-analytics/ea-assignments/ea-lst-capstone/exploratory/FH_Hydrosat.py:514\u001b[0m, in \u001b[0;36mFH_Hydrosat.point_time_series_from_items\u001b[0;34m(self, pt, tol, nproc, pad, band)\u001b[0m\n\u001b[1;32m    511\u001b[0m point_df \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39mGeoDataFrame({\u001b[39m'\u001b[39m\u001b[39mgeometry\u001b[39m\u001b[39m'\u001b[39m:[pt]}, crs\u001b[39m=\u001b[39mCRS\u001b[39m.\u001b[39mfrom_epsg(\u001b[39m4326\u001b[39m))\n\u001b[1;32m    513\u001b[0m \u001b[39m# project the point to raster CRS\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m ds \u001b[39m=\u001b[39m rxr\u001b[39m.\u001b[39mopen_rasterio(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitem_href[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    515\u001b[0m raster_crs \u001b[39m=\u001b[39m CRS\u001b[39m.\u001b[39mfrom_wkt(ds\u001b[39m.\u001b[39mspatial_ref\u001b[39m.\u001b[39mcrs_wkt)\n\u001b[1;32m    516\u001b[0m point_df_utm \u001b[39m=\u001b[39m point_df\u001b[39m.\u001b[39mto_crs(raster_crs)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Collection to search for.\n",
    "collection = [\"starfm_predictions_modis_landsat\", \"pydms_sharpened_landsat\"]\n",
    "asset = 'lst'\n",
    "\n",
    "# Search STAC for available images.\n",
    "(found_items, num_tiles) = stac_tile_search(collection, geom, start_date, end_date)\n",
    "\n",
    "# Extract data from AOI. \n",
    "# (aoi_lst_da) = create_aoi_image_stack(found_items, num_tiles, geom, asset, buffer)\n",
    "print (\"----\", type(bbox))\n",
    "# Extract data from point.\n",
    "(lst_time_series_df) = (extract_time_series(found_items, bbox, int(40), 'fused_lst', asset, int(0)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get AOI mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean of all pixels for each time-step.\n",
    "aoi_mean_lst_da = aoi_lst_da.isel(band=0).mean(dim=[\"x\", \"y\"])\n",
    "\n",
    "# Convert to dataframe.\n",
    "aoi_mean_lst_df = aoi_mean_lst_da.to_dataframe(name='starfm_lst')\n",
    "# Change date format to be consistent.\n",
    "aoi_mean_lst_df.index = pd.to_datetime(aoi_mean_lst_df.index)\n",
    "aoi_mean_lst_df.index = aoi_mean_lst_df.index.strftime('%Y-%m-%d')\n",
    "\n",
    "# Create df with a complete list of dates.\n",
    "days = pd.date_range(start, end)\n",
    "dates_df = pd.DataFrame({'time': days})\n",
    "dates_df['time'] = pd.to_datetime(dates_df.time)\n",
    "dates_df.set_index('time', inplace=True)\n",
    "# Change date format to be consistent.\n",
    "dates_df.index = dates_df.index.strftime('%Y-%m-%d')\n",
    "\n",
    "# Merge each df into master dataframe.\n",
    "# Merge on index (date) and keep all rows from both dfs (inner join).\n",
    "lst_df = pd.merge(\n",
    "    dates_df, aoi_mean_lst_df, \n",
    "    left_index=True, right_index=True, how = 'outer')  \n",
    "\n",
    "# Interpolate missing values.\n",
    "interpd_aoi_lst_df = lst_df.interpolate(method='linear', inplace=False)\n",
    "# Drop uneeded columns.\n",
    "interpd_aoi_lst_df.drop(columns=['band', 'spatial_ref'], inplace=True)\n",
    "\n",
    "# Plot up a time series of of the daily AOI mean LST.\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax = plt.plot(interpd_aoi_lst_df.starfm_lst, marker='o', markersize=4, c='blue')\n",
    "\n",
    "plt.title('AOI Mean of Fused LST Time Series')\n",
    "plt.grid(True)\n",
    "plt.ylabel('Fused LST [K] (20 m)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HRRR 2-m Temperature data and put it in it's own dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hrrr 2mT and Td data.\n",
    "hrrr, hrrr_td = get_hrrr_point_data(lat, lon, start, num_days)\n",
    "\n",
    "# Convert to dataframe and clean it up.\n",
    "# Get a dataset of point hrrr and convert to dataframe for easier use. \n",
    "hrrr_df = hrrr.to_dataframe()\n",
    "hrrr_td_df = hrrr_td.to_dataframe()\n",
    "\n",
    "# Make date consistent in format.\n",
    "hrrr_df.reset_index(inplace=True)\n",
    "hrrr_df['date'] = pd.to_datetime(hrrr_df['time'].dt.date)\n",
    "\n",
    "hrrr_td_df.reset_index(inplace=True)\n",
    "hrrr_td_df['date'] = pd.to_datetime(hrrr_td_df['time'].dt.date)\n",
    "\n",
    "# Set the index to date just like the other dataframes.\n",
    "hrrr_df.set_index('time', inplace=True)\n",
    "# Change date format to be consistent.\n",
    "hrrr_df.index = hrrr_df.index.strftime('%Y-%m-%d')\n",
    "\n",
    "# Set the index to date just like the other dataframes.\n",
    "hrrr_td_df.set_index('time', inplace=True)\n",
    "# Change date format to be consistent.\n",
    "hrrr_td_df.index = hrrr_td_df.index.strftime('%Y-%m-%d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Surface met obs from Synoptic Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = 'KAMW'\n",
    "obs_df = get_synoptic_obs(station)\n",
    "\n",
    "# Create df with a complete list of dates.\n",
    "days = pd.date_range(start, end)\n",
    "dates_df = pd.DataFrame({'time': days})\n",
    "dates_df['time'] = pd.to_datetime(dates_df.time)\n",
    "dates_df.set_index('time', inplace=True)\n",
    "# Change date format to be consistent.\n",
    "dates_df.index = dates_df.index.strftime('%Y-%m-%d')\n",
    "\n",
    "met_obs_df = pd.merge(\n",
    "    dates_df, obs_df, \n",
    "    left_index=True, right_index=True, how = 'outer')  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we'll calculate CATD using the LST and 2-m Temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATD will be calculated as:\n",
    "# (CATD = LST - 2 m Temperature)\n",
    "\n",
    "# Calculate CATD.\n",
    "catd_df = pd.DataFrame()\n",
    "catd_df['catd'] = interpd_aoi_lst_df['starfm_lst'].sub(hrrr_df['t2m'])\n",
    "\n",
    "# Plot up a time series of of the daily AOI mean LST.\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax = plt.plot(catd_df, marker='s', markersize=8, c='green')\n",
    "\n",
    "plt.title('CATD Time Series for All Pixels')\n",
    "plt.grid(True)\n",
    "plt.ylabel('CATD (20 m)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface reflectance is extracted for and NDVI is calculated.\n",
    "    \n",
    "# Search STAC for available images from collection..\n",
    "collection = \"starfm_predictions_modis_s2\"\n",
    "(found_items, num_tiles) = stac_tile_search(collection, geom, start_date, end_date)\n",
    "\n",
    "# Extract red band data from aoi.. \n",
    "asset='surface_reflectance_red'\n",
    "(aoi_red_da) = create_aoi_image_stack(found_items, num_tiles, geom, asset, buffer)\n",
    "\n",
    "# Extract nir band data from aoi. \n",
    "asset='surface_reflectance_nir'\n",
    "(aoi_nir_da) = create_aoi_image_stack(found_items, num_tiles, geom, asset, buffer)\n",
    "\n",
    "# Get mean of all pixels for each time-step.\n",
    "aoi_mean_red_da = aoi_red_da.isel(band=0).mean(dim=[\"x\", \"y\"])\n",
    "aoi_mean_nir_da = aoi_nir_da.isel(band=0).mean(dim=[\"x\", \"y\"])\n",
    "\n",
    "# Calculate ndvi.\n",
    "ndvi_da = (aoi_mean_nir_da - aoi_mean_red_da)  / (aoi_mean_nir_da + aoi_mean_red_da)\n",
    "\n",
    "# Create our own dataframe instead of using to_dataframe which takes\n",
    "# forever dealing with the dask graphs.\n",
    "# ndvi_vals = ndvi_da.values()\n",
    "# ndvi_dates = ndvi_da.time().values\n",
    "# ndvi_df = pd.DataFrame({\"time\": ndvi_dates, \"ndvi\": ndvi_vals})\n",
    "\n",
    "\n",
    "# Convert to dataaframe\n",
    "ndvi_df = ndvi_da.to_dataframe(name='ndvi')\n",
    "\n",
    "# Change date format to be consistent.\n",
    "# Set the index to date just like the other dataframes.\n",
    "\n",
    "# Change date format to be consistent.\n",
    "ndvi_df.index = ndvi_df.index.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Mean CATD and Mean NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot up a time series of of the daily AOI mean LST.\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "# Plot on 2 y-axes.\n",
    "ax2 = ax1.twinx()  \n",
    "\n",
    "ax1.plot(catd_df.index, catd_df.catd, marker='s', markersize=8, c='green', label='CATD')\n",
    "ax2.plot(ndvi_df.index, ndvi_df.ndvi, marker='s', markersize=8, c='blue', label='NDVI')\n",
    "\n",
    "#ax1.set_ylabel(\"Fused CATD\", color='green', fontsize=14)\n",
    "#ax2.set_ylabel(label=label, color='blue', fontsize=14)\n",
    "ax1.set(ylim=(-25, 25), ylabel=\"CATD\")\n",
    "ax2.set( ylabel=\"NDVI\")\n",
    "\n",
    "ax1.grid(True) \n",
    "\n",
    "plt.title('AOI Mean CATD & Mean NDVI')\n",
    "plt.xticks(rotation=45)\n",
    "ax1.set_xticklabels(ax1.get_xticks(), rotation = 45)\n",
    "\n",
    "ax1.legend(loc=2)\n",
    "ax2.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWSI-EB (Crop Water Stress Index) - Katimbo et al (2022) in Agricultural Water Management.\n",
    "\n",
    "CWSI = ((Tc - Ta)a - (Tcl - Ta)U) / ((Tcu - Ta)L - (Tcl - Ta)L)\n",
    "\n",
    "    Tc = canopy Temp (fused LST)\n",
    "    Ta = ambient air temp (HRRR 2mT)\n",
    "    Tcl = Tc of well transpiring veg (from aoi.max)\n",
    "    Tcu = Tc of non-transpiring veg (from aoi.min)\n",
    "    \n",
    "    (Tcl - Ta)L = A + B * VPD\n",
    "\n",
    "        VPD = e - es\n",
    "        e = 6.11 × 10 exp(7.5 × Td / 237.3 + Td )\n",
    "        es = 6.11 × 10 exp(7.5 × T / 237.3 + T )\n",
    "        A = intercept of a regression of (Tc - Ta) and VPD\n",
    "        B = slope of a regression of (Tc - Ta) and VPD\n",
    "\n",
    "    (Tc - Ta)a = A + B * VPG\n",
    "    \n",
    "        VPG = Ta - (Ta + A)\n",
    "        A = intercept of a regression of (Tc - Ta) and VPD\n",
    "        B = slope of a regression of (Tc - Ta) and VPD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "t_canopy_da = aoi_lst_da\n",
    "t_air = hrrr_df['t2m']\n",
    "t_d = hrrr_df['td']\n",
    "t_canopy_mx = aoi_lst_da.max()\n",
    "t_canopy_mn = aoi_lst_da.min()\n",
    "\n",
    "e = 6.11e((7.5 * t_d) / (273.3 + t_d))\n",
    "es = 6.11e((7.5 * t_air) / (273.3 + t_air))\n",
    "vpd = e - es\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(t_canopy_da, vpd)\n",
    "model = LinearRegression().fit(t_canopy_da, vpd)\n",
    "r_sq = model.score(t_canopy_da, vpd)\n",
    "a = model.intercept_\n",
    "b = model.coef_\n",
    "\n",
    "vpg = t_air - (t_air + a)\n",
    "\n",
    "cwsi = ((a + b * vpg) - (t_canopy_mx - t_air)) / ((t_canopy_mn - t_air) - (a + b * vpd))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate VPD and plot CATD vs. VPD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Vapor Pressure Deficit (VPD).\n",
    "# e = vapor pressure\n",
    "# es = saturated vapor pressure.\n",
    "e = 6.11 * 10 ** ((7.5 * (met_obs_df['dew_point_temperature'])) / (237.3 + (met_obs_df['dew_point_temperature'])))\n",
    "es = 6.11 * 10 ** ((7.5 * (met_obs_df['air_temp'])) / (237.3 + (met_obs_df['air_temp'])))\n",
    "vpd = es - e\n",
    "\n",
    "# Convert LST from K to C.\n",
    "c_lst_df = lst_df['starfm_lst'] - 273.15\n",
    "\n",
    "# Calculate CATD this time in celcius.\n",
    "catd_obs_df = c_lst_df.sub(met_obs_df['air_temp'])\n",
    "\n",
    "# Plot CATD vs. VPD\n",
    "plt.scatter(vpd['2020-07-01':'2020-08-30'], catd_obs_df['2020-07-01':'2020-08-30'], c='blue', alpha=0.5)\n",
    "plt.xlabel('Vaopr Pressure Deficit (VPD)')\n",
    "plt.ylabel('CATD')\n",
    "plt.title('AMES - CATD (Mean of 500 sqm AOI) vs. Observed VPD')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set coords for insitu met sites and crop field center points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "insitu_df = pd.DataFrame.from_dict(insitu_points,\n",
    "                                   orient='index',\n",
    "                                   columns=['lat', 'lon'])\n",
    "insitu_gdf = gpd.GeoDataFrame(\n",
    "    insitu_df, \n",
    "    geometry=gpd.points_from_xy(insitu_df.lon, insitu_df.lat),\n",
    "    crs=CRS.from_epsg(4326))\n",
    "insitu_gdf.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths to met data csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to data\n",
    "data_path = os.path.join('../data', 'Ameriflux')\n",
    "bi1_data_path = os.path.join(data_path, \n",
    "                             'AMF_US-Bi1_BASE-BADM_8-5',\n",
    "                             'AMF_US-Bi1_BASE_HH_8-5.csv')\n",
    "bi2_data_path = os.path.join(data_path,\n",
    "                             'AMF_US-Bi2_BASE-BADM_13-5',\n",
    "                             'AMF_US-Bi2_BASE_HH_13-5.csv')\n",
    "ds3_data_path = os.path.join(data_path,\n",
    "                             'AMF_US-DS3_BASE-BADM_1-5',\n",
    "                             'AMF_US-DS3_BASE_HH_1-5.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Temp and Vapor Pressure from insitsu site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start\n",
       "2016-08-13 10:30:00    27.80\n",
       "2016-08-14 10:30:00    26.57\n",
       "2016-08-15 10:30:00    22.47\n",
       "2016-08-16 10:30:00    23.88\n",
       "2016-08-17 10:30:00    25.63\n",
       "                       ...  \n",
       "2022-05-22 10:30:00    25.31\n",
       "2022-05-23 10:30:00    27.15\n",
       "2022-05-24 10:30:00    29.09\n",
       "2022-05-25 10:30:00    30.94\n",
       "2022-05-26 10:30:00    20.55\n",
       "Name: TA, Length: 2113, dtype: float64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 'TA' and 'VPD_PI'\n",
    "\n",
    "# AMF_US-Bi1_BASE_HH_8-5.csv\n",
    "bi1_df = read_ameriflux(bi1_data_path)\n",
    "# AMF_US-Bi2_BASE_HH_13-5\n",
    "bi2_df = read_ameriflux(bi2_data_path)\n",
    "# AMF_US-DS3_BASE-BADM_1-5\n",
    "bi3_df = read_ameriflux(ds3_data_path)\n",
    "\n",
    "bi1_df.index = pd.to_datetime(bi1_df.index)\n",
    "\n",
    "#bi1_df.TA\n",
    "#bi1_df.VPD_PI\n",
    "\n",
    "# Only want obs at 10:30 local time which are when the \n",
    "# sat images used for computing fused LST.\n",
    "met_fused_time_df = bi1_df.between_time('10:00','11:00', inclusive='neither')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search selected collections\n",
    "collections = [\"starfm_predictions_modis_landsat\", \"pydms_sharpened_landsat\"]\n",
    "start_date = \"2016-08-13T00:00:00Z\"\n",
    "end_date = \"2022-05-26T00:00:00Z\"\n",
    "\n",
    "search = catalog.search(\n",
    "    collections = collections,\n",
    "    intersects = box(*aoi_gdf.to_crs(CRS.from_epsg(4326)).total_bounds),\n",
    "    datetime = [start_date, end_date],\n",
    "    max_items = 500\n",
    ")\n",
    "returned_items = search.get_all_items()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrosat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dd708af889ccc3bd37f2e364356efaa19b67cae4fd5543545686cdc77cf6309"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
